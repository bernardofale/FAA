{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "\n",
    "# flatten the X matrix\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# # normalize the data\n",
    "# min_value = np.min(X)\n",
    "# max_value = np.max(X)\n",
    "# X = ((X - min_value) / (max_value - min_value)) * 255\n",
    "# X = np.round(X).astype(int)\n",
    "\n",
    "Y = np.zeros((X.shape[0], 10))\n",
    "Y[0:204, 9] = 1\n",
    "Y[204:409, 0] = 1\n",
    "Y[409:615, 7] = 1\n",
    "Y[615:822, 6] = 1\n",
    "Y[822:1028, 1] = 1\n",
    "Y[1028:1236, 8] = 1\n",
    "Y[1236:1443, 4] = 1\n",
    "Y[1443:1649, 3] = 1\n",
    "Y[1649:1855, 2] = 1\n",
    "Y[1855:, 5] = 1\n",
    "\n",
    "indices = np.argmax(Y, axis=1)\n",
    "Y = np.expand_dims(indices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "\n",
    "data = np.concatenate((Y, X), axis=1)\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "Y = data[:, 0]\n",
    "X = data[:, 1:]\n",
    "\n",
    "X_train = X[0:2000]\n",
    "Y_train = Y[0:2000]\n",
    "\n",
    "X_test = X[2000:]\n",
    "Y_test = Y[2000:]\n",
    "\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    \"\"\"\n",
    "    randomly initializes the weights of a layer with L_in incoming connections and L_out outgoing connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    epi = (6**1/2) / (L_in + L_out)**1/2\n",
    "    \n",
    "    W = np.random.rand(L_out,L_in+1) *(2*epi) -epi\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 4096\n",
    "hidden_layer_size = 25\n",
    "hidden_layer_size2 = 25\n",
    "num_labels = 10\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, hidden_layer_size2)\n",
    "initial_Theta3 = randInitializeWeights(hidden_layer_size2, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the gradient of sigmoid function\n",
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    computes the gradient of the sigmoid function\n",
    "    \"\"\"\n",
    "    sigGrad = sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "    return sigGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(theta1, theta2, theta3, input_layer_size, hidden_layer_size, hidden_layer_size2, num_labels,X, y,Lambda):\n",
    "    \"\"\"\n",
    "    nn_params contains the parameters unrolled into a vector\n",
    "    \n",
    "    compute the cost and gradient of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0] # number of training examples = 2000\n",
    "    J = 0\n",
    "    y10 = np.zeros((m, num_labels)) # 2000x10\n",
    "    for i in range(num_labels):\n",
    "        y10[:,i][:,np.newaxis] = np.where(y==i,1,0)\n",
    "\n",
    "    X = np.hstack((np.ones((m,1)),X)) # 2000x4097\n",
    "\n",
    "    a1 = sigmoid(X @ theta1.T) # 2000x4097 * 4097x10 = 2000x10\n",
    "    a1 = np.hstack((np.ones((m,1)), a1)) # hidden layer -> 2000x11\n",
    "    a2 = sigmoid(a1 @ theta2.T) # output layer -> 2000x11 * 11x10 = 2000x10\n",
    "\n",
    "    a2 = np.hstack((np.ones((m,1)), a2)) # hidden layer -> 2000x11\n",
    "\n",
    "    a3 = sigmoid(a2 @ theta3.T) # output layer -> 2000x11 * 11x10 = 2000x10\n",
    "\n",
    "\n",
    "    for j in range(num_labels):\n",
    "        J = J + sum(-y10[:,j] * np.log(a3[:,j]) - (1-y10[:,j])*np.log(1-a3[:,j]))\n",
    "\n",
    "    cost = 1/m * J\n",
    "    reg_J = cost + Lambda/(2*m) * (np.sum(theta1[:,1:]**2) + np.sum(theta2[:,1:]**2) + np.sum(theta3[:,1:]**2))\n",
    "\n",
    "    # Implement the backpropagation algorithm to compute the gradients\n",
    "\n",
    "    grad1 = np.zeros((theta1.shape)) # 10x4097\n",
    "    grad2 = np.zeros((theta2.shape)) # 10x11\n",
    "    grad3 = np.zeros((theta3.shape)) # 10x11\n",
    "\n",
    "    for i in range(m):\n",
    "        xi = X[i,:]\n",
    "        a1i = a1[i,:]\n",
    "        a2i = a2[i,:]\n",
    "        a3i = a3[i,:]\n",
    "        d3 = a3i - y10[i,:]\n",
    "        d2 = theta3.T @ d3.T * sigmoidGradient(np.hstack((1,xi @ theta1.T)))\n",
    "        d1 = theta2.T @ d2[1:] * sigmoidGradient(np.hstack((1,xi @ theta1.T)))\n",
    "        grad1 = grad1 + d1[1:][:,np.newaxis] @ xi[:,np.newaxis].T\n",
    "        grad2 = grad2 + d2[1:][:,np.newaxis] @ a1i[:,np.newaxis].T\n",
    "        grad3 = grad3 + d3.T[:,np.newaxis] @ a2i[:,np.newaxis].T\n",
    "\n",
    "    grad1 = 1/m * grad1\n",
    "    grad2 = 1/m * grad2\n",
    "    grad3 = 1/m * grad3\n",
    "\n",
    "    grad1_reg = grad1 + (Lambda/m) * np.hstack((np.zeros((theta1.shape[0],1)),theta1[:,1:]))\n",
    "    grad2_reg = grad2 + (Lambda/m) * np.hstack((np.zeros((theta2.shape[0],1)),theta2[:,1:]))\n",
    "    grad3_reg = grad3 + (Lambda/m) * np.hstack((np.zeros((theta3.shape[0],1)),theta3[:,1:]))\n",
    "\n",
    "    return cost, grad1, grad2, grad3, reg_J, grad1_reg, grad2_reg, grad3_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentnn(X,y,initial_theta1, initial_theta2, initial_theta3, alpha,num_iters,Lambda,input_layer_size, input_layer_size2, hidden_layer_size, num_labels):\n",
    "    \"\"\"\n",
    "    Take in numpy arra\n",
    "    y X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    theta1 = initial_theta1\n",
    "    theta2 = initial_theta2\n",
    "    theta3 = initial_theta3\n",
    "    J_history = []\n",
    "\n",
    "    # v1 = np.zeros(theta1.shape)\n",
    "    # v2 = np.zeros(theta2.shape)\n",
    "    # v3 = np.zeros(theta3.shape)\n",
    "\n",
    "    momentum = 0.8\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # theta1_head = theta1 - momentum * v1\n",
    "        # theta2_head = theta2 - momentum * v2\n",
    "        # theta3_head = theta3 - momentum * v3\n",
    "\n",
    "        cost, grad1, grad2, rad3, reg_J, grad1_reg, grad2_reg, grad3_reg = nnCostFunction(theta1, theta2, theta3, input_layer_size, hidden_layer_size, hidden_layer_size2, num_labels,X,y,Lambda)\n",
    "        # cost, grad1, grad2, rad3, reg_J, grad1_reg, grad2_reg, grad3_reg = nnCostFunction(theta1_head, theta2_head, theta3_head, input_layer_size, hidden_layer_size, hidden_layer_size2, num_labels,X,y,Lambda)\n",
    "        # theta1 = theta1 - (alpha * grad1)\n",
    "        # theta2 = theta2 - (alpha * grad2)\n",
    "\n",
    "        theta1 = theta1 - (alpha * grad1_reg)\n",
    "        theta2 = theta2 - (alpha * grad2_reg)\n",
    "        theta3 = theta3 - (alpha * grad3_reg)\n",
    "\n",
    "        # v1 = momentum * v1 + alpha * grad1_reg\n",
    "        # v2 = momentum * v2 + alpha * grad2_reg\n",
    "        # v3 = momentum * v3 + alpha * grad3_reg\n",
    "\n",
    "        # theta1 = theta1 - v1\n",
    "        # theta2 = theta2 - v2\n",
    "        # theta3 = theta3 - v3\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Cost at iteration',i,':',cost)\n",
    "            print('Regularized cost at iteration',i,':',reg_J)\n",
    "\n",
    "            # alpha = alpha * 0.9\n",
    "            # print('Learning rate at iteration',i,':',alpha)\n",
    "            \n",
    "        # J_history.append(cost)\n",
    "        J_history.append(reg_J)\n",
    "\n",
    "    return theta1, theta2, theta3, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0 : 6.933957056286354\n",
      "Regularized cost at iteration 0 : 6.934042552551208\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "num_iters = 1000\n",
    "Lambda = 1\n",
    "num_labels = 10\n",
    "\n",
    "theta1, theta2, theta3, J_history = gradientDescentnn(X_train,Y_train,initial_Theta1, initial_Theta2, initial_Theta3,alpha,num_iters,Lambda,input_layer_size, hidden_layer_size, hidden_layer_size2, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost function evolution during training.\n",
    "#In order to say learning has finished, the cost function has to converge to a flat rate\n",
    "plt.plot(J_history)  #\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Cost function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta1, theta2, theta3, X):\n",
    "    \"\"\"\n",
    "    Predict the label of an input given a trained neural network\n",
    "    \"\"\"\n",
    "    #number of training examples\n",
    "    m= len(X)\n",
    "        \n",
    "    # add an extra column of 1´s corresponding to xo=1\n",
    "    X = np.append(np.ones((m,1)),X,axis=1)\n",
    "    \n",
    "    #Compute the output of the hidden layer (with sigmoid activation functions)\n",
    "    z1=np.dot(X, theta1.T)  #Inputs to the hidden layer neurons\n",
    "    a1=sigmoid(z1)  #Outputs  of the hidden layer neurons\n",
    "    \n",
    "    #Add a column of ones\n",
    "    a1 = np.append(np.ones((m,1)),a1, axis=1)\n",
    "    \n",
    "    #Compute the output of the output layer (with sigmoid activation functions)\n",
    "    z2=np.dot(a1, theta2.T) #Inputs to the output layer neurons\n",
    "    a2=sigmoid(z2)  #Outputs  of the output layer neurons\n",
    "\n",
    "    #Add a column of ones\n",
    "    a2 = np.append(np.ones((m,1)),a2, axis=1)\n",
    "\n",
    "    #Compute the output of the output layer (with sigmoid activation functions)\n",
    "    z3=np.dot(a2, theta3.T) #Inputs to the output layer neurons\n",
    "    a3=sigmoid(z3)  #Outputs  of the output layer neurons\n",
    "\n",
    "    #Predict the class\n",
    "    return np.argmax(a3,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(theta1, theta2, theta3, X_test)\n",
    "\n",
    "print('Training Set Accuracy: {:.1f}%'.format(np.mean(pred == Y_test.ravel())*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
